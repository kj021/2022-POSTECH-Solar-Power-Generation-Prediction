{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5121d301",
      "metadata": {
        "id": "5121d301"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from pandas.io.json import json_normalize\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import xgboost as xgb\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from scipy.stats import randint\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0bdd8ccc",
      "metadata": {
        "id": "0bdd8ccc"
      },
      "outputs": [],
      "source": [
        "_API_URL = 'https://research-api.dershare.xyz'\n",
        "# _API_KEY 직접 입력\n",
        "_API_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJoSDROSE5yNXFiTjV4RmpBRzdHNFo4IiwiaWF0IjoxNjY4MDg3MDE4LCJleHAiOjE2Njg3ODM2MDAsInR5cGUiOiJhcGlfa2V5In0.YlteX4xVaN-Z8KC40qWaQ4J6vZqyd7iqMFgwMAHlBFo'\n",
        "_AUTH_PARAM = {'headers': {'Authorization': f'Bearer {_API_KEY}'}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1a1cff3a",
      "metadata": {
        "id": "1a1cff3a"
      },
      "outputs": [],
      "source": [
        "class make_dataset:\n",
        "    def __init__(self, weather_info_csv, num):\n",
        "        self.weather_info_csv = weather_info_csv\n",
        "        self.num = num\n",
        "        \n",
        "    def make_table(self):\n",
        "        weather_info_csv = self.weather_info_csv\n",
        "        num = self.num\n",
        "        df_electronic = pd.read_csv('gens.csv')\n",
        "        df_electronic['time'] = df_electronic['time'].str.split('+').str[0]\n",
        "        df_electronic['time'] = pd.to_datetime(df_electronic['time'])\n",
        "        df_weather = pd.read_csv(weather_info_csv)\n",
        "        df_weather = df_weather.rename(columns = {'id' : 'weather_id'})\n",
        "        df_weather['time'] = df_weather['time'].str.split('+').str[0]\n",
        "        df_weather['time'] = pd.to_datetime(df_weather['time'])\n",
        "        df_num = df_electronic[df_electronic['id'] == num]\n",
        "        df_weather['time'] = df_weather['time'].dt.round(freq = 'H')  \n",
        "        df_weather.drop_duplicates(['time'], inplace = True)\n",
        "        df_info = pd.merge(df_weather,df_num, how='outer')\n",
        "        df_info = df_info.dropna()\n",
        "        df_info = df_info.drop(['id'], axis = 1)\n",
        "        return df_info\n",
        "    \n",
        "    def concat_table(self):\n",
        "        df_info = self.make_table()\n",
        "        df_11 = make_dataset('발전소11_기상정보.csv', 11)\n",
        "        df_11 = df_11.make_table()\n",
        "        df_12 = make_dataset('발전소12_기상정보.csv', 12)\n",
        "        df_12 = df_12.make_table()\n",
        "        df_13 = make_dataset('발전소13_기상정보.csv', 13)\n",
        "        df_13 = df_13.make_table()\n",
        "        df_14 = make_dataset('발전소14_기상정보.csv', 14)\n",
        "        df_14 = df_14.make_table()    \n",
        "        df_train_table = pd.concat([df_11, df_12, df_13, df_14])\n",
        "        df_train_table = df_train_table.dropna()\n",
        "        return df_train_table\n",
        "    \n",
        "    def scaling_train_data(self):\n",
        "        df_train_table = self.concat_table()\n",
        "        x = df_train_table.loc[:, 'temperature' : 'precip_1h']\n",
        "        y = df_train_table.iloc[:, -1]\n",
        "        scaler = StandardScaler()\n",
        "        x = scaler.fit_transform(x)\n",
        "        y = np.array(y).reshape(-1, 1)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 1) \n",
        "        return X_train, X_test, y_train, y_test\n",
        "    \n",
        "    def weather_prediction_table(self):\n",
        "        id = 1\n",
        "        date = '2022-11-12' # 매일매일 수정해야한다.\n",
        "        hour = 4\n",
        "        forecasts_1 = requests.get(f'https://research-api.dershare.xyz/open-proc/cmpt-2022/weathers/1/{id}/forecasts/{date}/{hour}', headers={'Authorization': f'Bearer {_API_KEY}'}).json()\n",
        "        table = pd.DataFrame(forecasts_1)\n",
        "        table = table.drop('fcst_time', axis = 1)\n",
        "        table['time'] = table['time'].str.split('+').str[0]\n",
        "        table['time'] = pd.to_datetime(table['time'])\n",
        "        return table\n",
        "    \n",
        "    def final_scaling(self):\n",
        "        table = self.weather_prediction_table()\n",
        "        final_x = table.loc[:, 'temperature' : 'precip_1h']\n",
        "        return final_x\n",
        "        \n",
        "    def running_model(self):\n",
        "        X_train, X_test, y_train, y_test = self.scaling_train_data()\n",
        "        base_models = [\n",
        "        ('XGB', xgb.XGBRegressor(subsample=0.6,n_estimators=1000,\n",
        "                                 min_child_weight=2,max_depth=5,\n",
        "                                 learning_rate=0.02,gamma=0.05)),\n",
        "        ('SVR',SVR()),\n",
        "        ('Random Forest',RandomForestRegressor(random_state =1, max_features = 5, \n",
        "                                               n_estimators = 103, max_depth = 189)),\n",
        "        ('MLP Regression',MLPRegressor(random_state=1, max_iter=500)),\n",
        "        ('Gradient Boostiong', GradientBoostingRegressor(learning_rate = 0.01, max_depth = 157, \n",
        "                                                         min_samples_split = 125, n_estimators = 166))]\n",
        "        stacked = StackingRegressor(\n",
        "        estimators = base_models,\n",
        "        final_estimator = LinearRegression(), cv = 5)\n",
        "    \n",
        "        for name, model in base_models:\n",
        "            start_time = time.time()\n",
        "            model.fit(X_train, y_train)\n",
        "            prediction = model.predict(X_test)\n",
        "            end_time = time.time()\n",
        "            r2 = model.score(X_test, y_test)\n",
        "            rmse = mean_squared_error(y_test, prediction, squared = False)\n",
        "            print(\"-------{}-------\".format(name))\n",
        "            print(\"Coefficient of determination: {}\".format(r2))\n",
        "            print(\"Root Mean Squared Error: {}\".format(rmse))\n",
        "            print(\"Computation Time: {}\".format(end_time - start_time))\n",
        "            print(\"----------------------------------\\n\")\n",
        "    \n",
        "        start_time = time.time()\n",
        "        stacked.fit(X_train, y_train)    \n",
        "        stacked_prediction = stacked.predict(X_test)\n",
        "        end_time = time.time()\n",
        "        stacked_r2 = stacked.score(X_test, y_test)\n",
        "        stacked_rmse = mean_squared_error(y_test, stacked_prediction, squared = False)\n",
        "        print(\"-------Stacked Ensemble-------\")\n",
        "        print(\"Coefficient of determination: {}\".format(stacked_r2))\n",
        "        print(\"Root Mean Squared Error: {}\".format(stacked_rmse))\n",
        "        print(\"Computation Time: {}\".format(end_time - start_time))\n",
        "        print(\"----------------------------------\")\n",
        "    \n",
        "    def final_prediction(self):\n",
        "        X_train, X_test, y_train, y_test = self.scaling_train_data()\n",
        "        final_x = self.final_scaling()\n",
        "        scaler = StandardScaler()\n",
        "        final_x = scaler.fit_transform(final_x)\n",
        "#        base_models = [\n",
        "#         ('XGB', xgb.XGBRegressor(subsample=0.6,n_estimators=1000,\n",
        "#                                  min_child_weight=2,max_depth=5,\n",
        "#                                  learning_rate=0.02,gamma=0.05)),\n",
        "#         ('SVR',SVR()),\n",
        "#         ('Random Forest',RandomForestRegressor(random_state =1, max_features = 5, \n",
        "#                                                n_estimators = 103, max_depth = 189)),\n",
        "#         ('MLP Regression',MLPRegressor(random_state=1, max_iter=500)),\n",
        "#         ('Gradient Boostiong', GradientBoostingRegressor(learning_rate = 0.01, max_depth = 157, \n",
        "#                                                          min_samples_split = 125, n_estimators = 166))]\n",
        "#        stacked = StackingRegressor(\n",
        "#        estimators = base_models,\n",
        "#        final_estimator = LinearRegression(), cv = 5)\n",
        "#        stacked.fit(X_train, y_train)    \n",
        "#        prediction = stacked.predict(final_x)\n",
        "#        return prediction        \n",
        "        model = xgb.XGBRegressor(subsample=0.6,n_estimators=1000,\n",
        "                                 min_child_weight=2,max_depth=5,\n",
        "                                 learning_rate=0.02,gamma=0.05)\n",
        "        model.fit(X_train, y_train)\n",
        "        prediction = model.predict(final_x)\n",
        "        return prediction\n",
        "    \n",
        "    def final_table(self):\n",
        "        prediction = self.final_prediction()\n",
        "        table = self.weather_prediction_table()\n",
        "        table['predict'] = prediction\n",
        "        return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ba086166",
      "metadata": {
        "id": "ba086166"
      },
      "outputs": [],
      "source": [
        "data = make_dataset('발전소11_기상정보.csv', 11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8e4050d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "id": "8e4050d2",
        "outputId": "23fa6060-5535-4b50-f6c1-2d3b40613fb6"
      },
      "outputs": [],
      "source": [
        "df1=data.final_table()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c88fd8e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "df1.to_csv(\"postech_1111예측.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe5ff3b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2b338e1ebdc85b878dc81ea058f20b659780bac2cb1a796931e4defa97a9d4b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
